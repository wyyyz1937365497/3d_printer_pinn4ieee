# Methodology Section Template (IEEE Format)

**Purpose**: Template for writing the methodology/system section of an IEEE paper.

---

## Structure Overview

```
III. METHODOLOGY

A. System Overview
   1. Problem formulation
   2. Overall architecture

B. Physics-Based Simulation
   1. Trajectory dynamics model
   2. Firmware effects

C. Neural Network Architecture
   1. LSTM architecture
   2. Loss function

D. Training Strategy
   1. Data generation
   2. Training procedure
   3. Hyperparameters
```

---

## A. System Overview

### 1. Problem Formulation

**Template**:
```
Let x(t) = [x(t), y(t)]^T denote the reference trajectory planned by the firmware. The actual trajectory r(t) = x(t) + e(t) deviates due to dynamic errors e(t). Our goal is to learn a function f(·) that predicts the error sequence given the reference trajectory:

    ê(t+1:t+T) = f(x(t-L:t-L), θ)

where L is the input sequence length, T is the prediction horizon, and θ are the learnable parameters.
```

**Key elements**:
- Clear mathematical notation
- Problem formulation
- Variables defined

### 2. Overall Architecture

**Template**:
```
Fig. 1 shows the overall system architecture. The system consists of three main components: (1) a physics-based simulator that generates training data by modeling printer dynamics and firmware behavior; (2) a neural network that learns to predict trajectory errors from reference trajectories; and (3) a real-time correction module that applies predicted errors to compensate the planned trajectory.

[Include figure here]

The physics simulator implements the second-order mass-spring-damper model described in Section III-B, while the neural network uses a lightweight LSTM architecture (Section III-C). Training data is generated by simulating diverse G-code files across a range of motion parameters (Section III-D).
```

**Key elements**:
- Reference to figure
- High-level component description
- Forward references to detailed sections

---

## B. Physics-Based Simulation

### 1. Trajectory Dynamics Model

**Template**:
```
The print head dynamics are modeled as a second-order mass-spring-damper system:

    mẍ(t) + cẋ(t) + kx(t) = -ma_ref(t)        (1)

where m is the effective mass (0.485 kg for X-axis), c is the damping coefficient (25 N·s/m), k is the belt stiffness (150,000 N/m), and a_ref(t) is the reference acceleration from the G-code planner.

The transfer function from acceleration to position error is:

    H(s) = X(s)/A_ref(s) = -1/(s² + 2ζωₙs + ωₙ²)        (2)

where ωₙ = √(k/m) is the natural frequency (556 rad/s for X-axis) and ζ = c/(2√(mk)) is the damping ratio (0.046 for X-axis).
```

**Key elements**:
- Equation numbers (in LaTeX: \eqref{eq:label})
- Parameter values in parentheses
- Clear variable definitions

### 2. Firmware Effects

**Template**:
```
Three firmware-level error sources are modeled:

1) Junction Deviation: At corners, the firmware reduces velocity to prevent overshoot. The corner velocity v_c is given by:

    v_c² = a·JD·sin(θ/2) / (1 - sin(θ/2))        (5)

where a is acceleration, JD is the junction deviation parameter, and θ is the corner angle.

2) Microstep Resonance: At 16× microstepping, resonance occurs at:

    f_res = f_motor / 16 ≈ 50 Hz        (6)

3) Timer Jitter: Interrupt overhead causes timing errors of:

    Δt_jitter = N_cycles / f_CPU ≈ 2.4 μs        (7)

These errors are superimposed on the basic dynamics using root-sum-square (RSS) combination.
```

---

## C. Neural Network Architecture

### 1. LSTM Architecture

**Template**:
```
The neural network processes the input sequence x ∈ ℝ^(L×F) where L = 20 is the sequence length and F = 4 is the number of features (x_ref, y_ref, vx_ref, vy_ref). We use a lightweight LSTM architecture with the following design:

- Feature encoder: Linear(4 → 32) with LayerNorm and ReLU activation
- Temporal modeling: 2-layer LSTM with 56 hidden units
- Dropout: p_drop = 0.1 for regularization
- Output projection: Linear(56 → 2) for error_x, error_y

The LSTM computes the hidden state sequence:

    h_t, c_t = LSTM(x_t, h_{t-1}, c_{t-1})        (8)

where h_t ∈ ℝ^56 is the hidden state and c_t ∈ ℝ^56 is the cell state at time t. The final prediction uses only the last hidden state:

    ê = W_o h_L + b_o        (9)

where ê ∈ ℝ^2 is the predicted error vector [error_x, error_y]^T.
```

**Key elements**:
- Architecture parameters clearly stated
- Equations for key operations
- Dimension notation (ℝ for real vector space)
- Single-step prediction (not sequence-to-sequence)

### 2. Loss Function

**Template**:
```
The model is trained to minimize the mean absolute error (MAE):

    L(θ) = (1/N) Σ_i |ê_i - e_i|        (13)

where N is the number of samples in the batch. To enforce physical constraints, we add a regularization term:

    L_physics = λ Σ_i (ê_i·v_i)²        (14)

which penalizes predictions that violate the correlation between error and velocity. The total loss is:

    L_total = L_MAE + λ·L_physics        (15)

We use λ = 0.1 determined via cross-validation.
```

---

## D. Training Strategy

### 1. Data Generation

**Template**:
```
Training data is generated using the physics simulator described in Section III-B. We use four test models (3DBenchy, Bearing5, Nautilus, Boat) and sample every 5th layer to balance diversity and computational cost. For each layer, we vary:
- Acceleration: {200, 300, 400, 500} mm/s²
- Velocity: {100, 200, 300, 400} mm/s

This yields 16 parameter combinations. Each simulation generates approximately 280 samples, resulting in a total of ~36,000 samples across all models. We apply data augmentation (temporal shifting, noise injection, and scaling) to increase dataset diversity.
```

**Key elements**:
- Specific parameter values
- Computation of total samples
- Mention of data augmentation

### 2. Training Procedure

**Template**:
```
The dataset is split into training (80%), validation (10%), and test (10%) sets. We use the AdamW optimizer [11] with:
- Initial learning rate: η = 10⁻⁴
- Weight decay: λ = 10⁻⁴
- Batch size: B = 32
- Betas: (β₁, β₂) = (0.9, 0.999)

The learning rate is reduced by a factor of 0.5 when the validation loss plateaus for 10 epochs (ReduceLROnPlateau scheduler). Training stops early if no improvement is observed for 15 epochs. We use mixed-precision training (FP16) to accelerate computation.

Training converges in approximately 50 epochs on an NVIDIA RTX 3080 GPU, taking about 2 hours.
```

### 3. Hyperparameter Selection

**Template**:
```
Table I shows the hyperparameter values used in our model. These were selected via grid search over the following ranges:
- hidden_size ∈ {32, 56, 128}
- num_layers ∈ {1, 2, 3}
- dropout ∈ {0.0, 0.1, 0.2}
- learning_rate ∈ {10⁻⁴, 10⁻³, 10⁻²}
- batch_size ∈ {128, 256, 512}

The selected configuration (hidden_size=56, num_layers=2, dropout=0.1, lr=10⁻³, batch_size=256) achieved the best validation MAE of 0.0156 mm with 38,193 total parameters.

[Insert Table I here: Hyperparameters and their values]
```

---

## Common Phrases

### Transitions

- "The system consists of three main components:"
- "Fig. X shows the overall architecture:"
- "We model this as a second-order system:"
- "The neural network architecture is shown in Fig. X:"
- "Training data is generated by:"

### Describing Equations

- "The dynamics are governed by:"
- "This can be expressed as:"
- "Substituting Eq. (X) into Eq. (Y) yields:"
- "Solving for X gives:"
- "Where X represents..."

### Referring to Figures/Tables

- "As shown in Fig. X:"
- "Table I summarizes:"
- "The architecture in Fig. 2 illustrates:"
- "Results in Table II demonstrate:"

---

## Writing Tips

### DO's ✅

1. **Number equations sequentially** throughout the paper
2. **Define all variables** when first introduced
3. **Use consistent notation** (e.g., vectors bold, scalars italic)
4. **Reference figures and tables** in the text
5. **Include parameter values** in parentheses
6. **Cite related work** for standard techniques

### DON'Ts ❌

1. **Don't omit equation numbers**: Every equation should be numbered
2. **Don't use ambiguous notation**: Be consistent with symbols
3. **Don't forget units**: Include units for all physical quantities
4. **Don't make figures too small**: IEEE columns are 3.5" wide
5. **Don't exceed page limits**: Methods is typically 2-3 pages

---

## LaTeX Template

```latex
\section{Methodology}
\label{sec:methodology}

\subsection{System Overview}
\label{subsec:overview}

The overall system architecture...

\subsection{Physics-Based Simulation}
\label{subsec:simulation}

\subsubsection{Trajectory Dynamics}
\label{subsubsec:dynamics}

The print head dynamics are modeled as:

\begin{equation}
m\ddot{x} + c\dot{x} + kx = -ma_{\text{ref}}(t)
\label{eq:dynamics}
\end{equation}

\subsection{Neural Network Architecture}
\label{subsec:network}

Fig.~\ref{fig:network} shows the network architecture...

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{network.pdf}
\caption{LSTM architecture for real-time trajectory error prediction.}
\label{fig:network}
\end{figure}

\subsection{Training Strategy}
\label{subsec:training}

Table~\ref{tab:hyperparams} shows the hyperparameters...

\begin{table}[t]
\centering
\caption{Model Hyperparameters}
\label{tab:hyperparams}
\begin{tabular}{lc}
\hline
Parameter & Value \\
\hline
hidden\_size & 56 \\
num\_layers & 2 \\
dropout & 0.1 \\
learning\_rate & 10$^{-3}$ \\
batch\_size & 256 \\
\hline
\end{tabular}
\end{table}
```

---

## Related Templates

- [Introduction](introduction.md) - Previous section
- [Experiments](experiments.md) - Next section
- [Results](results.md) - Results section template
- [Conclusion](conclusion.md) - Conclusions template

---

**Last Updated**: 2026-02-02
