# Methodology Section Template (IEEE Format)

**Purpose**: Template for writing the methodology/system section of an IEEE paper.

---

## Structure Overview

```
III. METHODOLOGY

A. System Overview
   1. Problem formulation
   2. Overall architecture

B. Physics-Based Simulation
   1. Trajectory dynamics model
   2. Thermal model
   3. Firmware effects

C. Neural Network Architecture
   1. Encoder design
   2. Decoder design
   3. Loss function

D. Training Strategy
   1. Data generation
   2. Training procedure
   3. Hyperparameters
```

---

## A. System Overview

### 1. Problem Formulation

**Template**:
```
Let x(t) = [x(t), y(t)]^T denote the reference trajectory planned by the firmware. The actual trajectory r(t) = x(t) + e(t) deviates due to dynamic errors e(t). Our goal is to learn a function f(·) that predicts the error sequence given the reference trajectory:

    ê(t+1:t+T) = f(x(t-L:t-L), θ)

where L is the input sequence length, T is the prediction horizon, and θ are the learnable parameters.
```

**Key elements**:
- Clear mathematical notation
- Problem formulation
- Variables defined

### 2. Overall Architecture

**Template**:
```
Fig. 1 shows the overall system architecture. The system consists of three main components: (1) a physics-based simulator that generates training data by modeling printer dynamics, thermal effects, and firmware behavior; (2) a neural network that learns to predict trajectory errors from reference trajectories; and (3) a real-time correction module that applies predicted errors to compensate the planned trajectory.

[Include figure here]

The physics simulator implements the second-order mass-spring-damper model described in Section III-B, while the neural network uses a Transformer encoder with LSTM decoder architecture (Section III-C). Training data is generated by simulating diverse G-code files across a range of process parameters (Section III-D).
```

**Key elements**:
- Reference to figure
- High-level component description
- Forward references to detailed sections

---

## B. Physics-Based Simulation

### 1. Trajectory Dynamics Model

**Template**:
```
The print head dynamics are modeled as a second-order mass-spring-damper system:

    mẍ(t) + cẋ(t) + kx(t) = -ma_ref(t)        (1)

where m is the effective mass (0.485 kg for X-axis), c is the damping coefficient (25 N·s/m), k is the belt stiffness (150,000 N/m), and a_ref(t) is the reference acceleration from the G-code planner.

The transfer function from acceleration to position error is:

    H(s) = X(s)/A_ref(s) = -1/(s² + 2ζωₙs + ωₙ²)        (2)

where ωₙ = √(k/m) is the natural frequency (556 rad/s for X-axis) and ζ = c/(2√(mk)) is the damping ratio (0.046 for X-axis).
```

**Key elements**:
- Equation numbers (in LaTeX: \eqref{eq:label})
- Parameter values in parentheses
- Clear variable definitions

### 2. Thermal Model (if applicable)

**Template**:
```
The temperature field evolution during printing is modeled using the heat conduction equation:

    ρc_p ∂T/∂t = k∇²T + q_source - q_cooling        (3)

For real-time computation, we use a simplified point-tracking approach. The temperature at layer n is computed as:

    T_n = 0.7·T_cooling(n) + 0.3·T_below(n)        (4)

where T_cooling accounts for convective cooling and T_below captures thermal diffusion from recent layers.
```

### 3. Firmware Effects

**Template**:
```
Three firmware-level error sources are modeled:

1) Junction Deviation: At corners, the firmware reduces velocity to prevent overshoot. The corner velocity v_c is given by:

    v_c² = a·JD·sin(θ/2) / (1 - sin(θ/2))        (5)

where a is acceleration, JD is the junction deviation parameter, and θ is the corner angle.

2) Microstep Resonance: At 16× microstepping, resonance occurs at:

    f_res = f_motor / 16 ≈ 50 Hz        (6)

3) Timer Jitter: Interrupt overhead causes timing errors of:

    Δt_jitter = N_cycles / f_CPU ≈ 2.4 μs        (7)

These errors are superimposed on the basic dynamics using root-sum-square (RSS) combination.
```

---

## C. Neural Network Architecture

### 1. Encoder Design

**Template**:
```
The encoder processes the input sequence x ∈ ℝ^(L×F) where L = 128 is the sequence length and F = 15 is the number of features (position, velocity, acceleration, jerk, curvature). We use a Transformer encoder [10] with the following architecture:

- Input embedding: Linear(F → d_model) with d_model = 256
- Positional encoding: Sinusoidal with frequency ω_k = 1/10000^(2k/d_model)
- N = 6 identical layers, each with:
  - Multi-head self-attention: H = 8 heads
  - Feed-forward network: d_ff = 1024 hidden units
  - Dropout: p_drop = 0.1
- Layer normalization and residual connections after each sub-layer

The self-attention mechanism computes:

    Attention(Q, K, V) = softmax(QK^T/√d_k)V        (8)

where Q, K, V are projections of the input with d_k = d_model/H = 32.
```

**Key elements**:
- Architecture parameters clearly stated
- Equations for key operations
- Citations for standard components
- Dimension notation (ℝ for real vector space)

### 2. Decoder Design

**Template**:
```
The decoder generates the error sequence ê ∈ ℝ^(T×2) using a bidirectional LSTM:

    h_t = LSTM(h_{t-1}, [z_enc; ê_{<t}])        (9)

where z_enc is the encoder output and ê_{<t} are previous predictions. We use 2 LSTM layers with 256 hidden units each. An attention mechanism over encoder outputs provides context for each prediction step:

    c_t = Σ_i α_{t,i} z_enc,i        (10)
    α_{t,i} = softmax(v^T tanh(W₁h_t + W₂z_enc,i))        (11)

The final prediction uses a linear projection:

    ê_t = W_o h_t + b_o        (12)
```

### 3. Loss Function

**Template**:
```
The model is trained to minimize the mean absolute error (MAE):

    L(θ) = (1/N) Σ_i |ê_i - e_i|        (13)

where N is the number of samples in the batch. To enforce physical constraints, we add a regularization term:

    L_physics = λ Σ_i (ê_i·v_i)²        (14)

which penalizes predictions that violate the correlation between error and velocity. The total loss is:

    L_total = L_MAE + λ·L_physics        (15)

We use λ = 0.1 determined via cross-validation.
```

---

## D. Training Strategy

### 1. Data Generation

**Template**:
```
Training data is generated using the physics simulator described in Section III-B. We use four test models (3DBenchy, Bearing5, Nautilus, Boat) and sample every 5th layer to balance diversity and computational cost. For each layer, we vary:
- Acceleration: {200, 300, 400, 500} mm/s²
- Velocity: {100, 200, 300, 400} mm/s
- Fan speed: {0, 128, 255} (0-100%)
- Ambient temperature: {20, 25, 30} °C

This yields 144 parameter combinations. Each simulation generates approximately 280 samples, resulting in a total of 40,320 samples. We apply data augmentation (random rotation, scaling, and time warping) to triple the dataset size to 120,960 samples.
```

**Key elements**:
- Specific parameter values
- Computation of total samples
- Mention of data augmentation

### 2. Training Procedure

**Template**:
```
The dataset is split into training (80%), validation (10%), and test (10%) sets. We use the AdamW optimizer [11] with:
- Initial learning rate: η = 10⁻⁴
- Weight decay: λ = 10⁻⁴
- Batch size: B = 32
- Betas: (β₁, β₂) = (0.9, 0.999)

The learning rate is reduced by a factor of 0.5 when the validation loss plateaus for 10 epochs (ReduceLROnPlateau scheduler). Training stops early if no improvement is observed for 15 epochs. We use mixed-precision training (FP16) to accelerate computation.

Training converges in approximately 50 epochs on an NVIDIA RTX 3080 GPU, taking about 2 hours.
```

### 3. Hyperparameter Selection

**Template**:
```
Table I shows the hyperparameter values used in our model. These were selected via grid search over the following ranges:
- d_model ∈ {128, 256, 512}
- N_layers ∈ {4, 6, 8}
- n_heads ∈ {4, 8, 16}
- dropout ∈ {0.0, 0.1, 0.2}
- learning_rate ∈ {10⁻⁵, 10⁻⁴, 10⁻³}

The selected configuration (d_model=256, N_layers=6, n_heads=8, dropout=0.1, lr=10⁻⁴) achieved the best validation MAE of 0.0151 mm.

[Insert Table I here: Hyperparameters and their values]
```

---

## Common Phrases

### Transitions

- "The system consists of three main components:"
- "Fig. X shows the overall architecture:"
- "We model this as a second-order system:"
- "The neural network architecture is shown in Fig. X:"
- "Training data is generated by:"

### Describing Equations

- "The dynamics are governed by:"
- "This can be expressed as:"
- "Substituting Eq. (X) into Eq. (Y) yields:"
- "Solving for X gives:"
- "Where X represents..."

### Referring to Figures/Tables

- "As shown in Fig. X:"
- "Table I summarizes:"
- "The architecture in Fig. 2 illustrates:"
- "Results in Table II demonstrate:"

---

## Writing Tips

### DO's ✅

1. **Number equations sequentially** throughout the paper
2. **Define all variables** when first introduced
3. **Use consistent notation** (e.g., vectors bold, scalars italic)
4. **Reference figures and tables** in the text
5. **Include parameter values** in parentheses
6. **Cite related work** for standard techniques

### DON'Ts ❌

1. **Don't omit equation numbers**: Every equation should be numbered
2. **Don't use ambiguous notation**: Be consistent with symbols
3. **Don't forget units**: Include units for all physical quantities
4. **Don't make figures too small**: IEEE columns are 3.5" wide
5. **Don't exceed page limits**: Methods is typically 2-3 pages

---

## LaTeX Template

```latex
\section{Methodology}
\label{sec:methodology}

\subsection{System Overview}
\label{subsec:overview}

The overall system architecture...

\subsection{Physics-Based Simulation}
\label{subsec:simulation}

\subsubsection{Trajectory Dynamics}
\label{subsubsec:dynamics}

The print head dynamics are modeled as:

\begin{equation}
m\ddot{x} + c\dot{x} + kx = -ma_{\text{ref}}(t)
\label{eq:dynamics}
\end{equation}

\subsection{Neural Network Architecture}
\label{subsec:network}

Fig.~\ref{fig:network} shows the network architecture...

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{network.pdf}
\caption{Neural network architecture for trajectory error prediction.}
\label{fig:network}
\end{figure}

\subsection{Training Strategy}
\label{subsec:training}

Table~\ref{tab:hyperparams} shows the hyperparameters...

\begin{table}[t]
\centering
\caption{Model Hyperparameters}
\label{tab:hyperparams}
\begin{tabular}{lc}
\hline
Parameter & Value \\
\hline
d_model & 256 \\
N_layers & 6 \\
n_heads & 8 \\
dropout & 0.1 \\
\hline
\end{tabular}
\end{table}
```

---

## Related Templates

- [Introduction](introduction.md) - Previous section
- [Experiments](experiments.md) - Next section
- [Results](results.md) - Results section template
- [Conclusion](conclusion.md) - Conclusions template

---

**Last Updated**: 2026-02-02
