"""
3D Printer Simulation Dataset from MATLAB Data

This module provides PyTorch Dataset classes for loading
MATLAB .mat files generated by the simulation pipeline.
"""

import os
import glob
import numpy as np
import torch
import scipy.io as sio
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Optional, Union
from sklearn.preprocessing import StandardScaler


class PrinterSimulationDataset(Dataset):
    """
    PyTorch Dataset for 3D printer simulation data from MATLAB

    This dataset loads MATLAB .mat files and converts them to
    tensors suitable for training the PINN model.

    Input Features (12 total):
        - Ideal trajectory (6): x_ref, y_ref, z_ref, vx_ref, vy_ref, vz_ref
        - Observable measurements (6): T_nozzle, T_interface,
          F_inertia_x, F_inertia_y, cooling_rate, layer_num

    Output Labels:
        - Trajectory error (2): error_x, error_y
        - Quality metrics (5): adhesion_ratio, internal_stress,
          porosity, dimensional_accuracy, quality_score
    """

    # Define input feature names (12 features)
    INPUT_FEATURES = [
        # Ideal trajectory (6)
        'x_ref', 'y_ref', 'z_ref',
        'vx_ref', 'vy_ref', 'vz_ref',
        # Observable measurements (6)
        'T_nozzle', 'T_interface',
        'F_inertia_x', 'F_inertia_y',
        'cooling_rate', 'layer_num'
    ]

    # Define output labels
    OUTPUT_TRAJECTORY = ['error_x', 'error_y']
    OUTPUT_QUALITY = [
        'adhesion_ratio',
        'internal_stress',
        'porosity',
        'dimensional_accuracy',
        'quality_score'
    ]

    def __init__(self,
                 data_files: Union[str, List[str]],
                 seq_len: int = 200,
                 pred_len: int = 50,
                 stride: int = 10,
                 mode: str = 'train',
                 scaler: Optional[StandardScaler] = None,
                 fit_scaler: bool = True):
        """
        Initialize dataset

        Args:
            data_files: Path to .mat file or list of paths
            seq_len: Input sequence length (timesteps)
            pred_len: Prediction sequence length
            stride: Stride between sequences
            mode: 'train', 'val', or 'test'
            scaler: Optional fitted StandardScaler
            fit_scaler: Whether to fit scaler on this data
        """
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.stride = stride
        self.mode = mode

        # Load data from files
        self.data_list = self._load_data(data_files)

        # Fit or use provided scaler
        if scaler is None and fit_scaler:
            self.scaler = StandardScaler()
            self._fit_scaler()
        else:
            self.scaler = scaler

        # Create sequences
        self.sequences = self._create_sequences()

        print(f"[{mode.upper()}] Loaded {len(self.sequences)} sequences "
              f"from {len(self.data_list)} files")

    def _load_data(self, data_files: Union[str, List[str]]) -> List[Dict]:
        """Load MATLAB .mat files"""
        if isinstance(data_files, str):
            # If it's a directory, find all .mat files
            if os.path.isdir(data_files):
                data_files = glob.glob(os.path.join(data_files, "*.mat"))
            else:
                data_files = [data_files]

        data_list = []
        for filepath in data_files:
            try:
                data = self._load_mat_file(filepath)
                if data is not None:
                    data_list.append(data)
            except Exception as e:
                print(f"Warning: Failed to load {filepath}: {e}")

        print(f"Loaded {len(data_list)} MATLAB files")
        return data_list

    def _load_mat_file(self, filepath: str) -> Optional[Dict]:
        """
        Load single MATLAB .mat file and extract features

        Supports both v7.3 (HDF5) and older formats.

        Returns:
            Dictionary with numpy arrays for each feature
        """
        # Try h5py first for v7.3 format
        try:
            import h5py
            with h5py.File(filepath, 'r') as mat_data:
                if 'simulation_data' not in mat_data:
                    print(f"Warning: {filepath} does not contain 'simulation_data'")
                    return None

                sim_data = mat_data['simulation_data']

                # Extract data - handle v7.3 format differently
                data = {}
                for field in sim_data.keys():
                    value = sim_data[field]
                    # Check if it's a reference to another dataset (common in v7.3)
                    if hasattr(value, 'shape') and len(value.shape) > 0:
                        # For MATLAB v7.3, values are stored transposed
                        if value.dtype.kind in ['c', 'f', 'i']:  # Complex, float, integer
                            # Read the actual data
                            actual_value = value[()]
                            # Transpose if needed (v7.3 stores data differently)
                            if len(actual_value.shape) == 2:
                                actual_value = np.transpose(actual_value)
                            data[field] = actual_value.flatten() if actual_value.size <= actual_value.shape[-1] else actual_value
                        elif value.dtype.kind == 'U':  # Unicode strings
                            data[field] = value[()].item().strip('\x00')
                        else:
                            # Handle reference to another dataset
                            if value.shape == () or len(value.shape) == 1:
                                # If it's a scalar reference
                                ref = value[()]
                                if hasattr(ref, 'shape'):
                                    data[field] = np.array(ref)
                                else:
                                    # It might be a reference to another dataset
                                    try:
                                        ref_data = mat_data[ref]
                                        if len(ref_data.shape) == 2:
                                            actual_value = np.transpose(ref_data[()])
                                        else:
                                            actual_value = ref_data[()]
                                        data[field] = actual_value.flatten() if actual_value.size <= actual_value.shape[-1] else actual_value
                                    except:
                                        # Skip if reference doesn't work
                                        continue
                            else:
                                # Multiple references
                                actual_values = []
                                for ref_item in value:
                                    try:
                                        ref_data = mat_data[ref_item]
                                        if len(ref_data.shape) == 2:
                                            actual_value = np.transpose(ref_data[()])
                                        else:
                                            actual_value = ref_data[()]
                                        actual_values.append(actual_value.flatten() if actual_value.size <= actual_value.shape[-1] else actual_value)
                                    except:
                                        continue
                                if actual_values:
                                    data[field] = np.hstack(actual_values)

        except (Exception,) as e:
            # Print exception for debugging
            print(f"Debug: h5py failed for {filepath}, trying scipy: {e}")
            # Fallback to scipy for older formats
            try:
                mat_data = sio.loadmat(filepath)

                if 'simulation_data' not in mat_data:
                    print(f"Warning: {filepath} does not contain 'simulation_data'")
                    return None

                sim_data = mat_data['simulation_data'][0, 0]

                # Extract data
                data = {}
                field_names = sim_data.dtype.names

                for field in field_names:
                    value = sim_data[field][0, 0]
                    if isinstance(value, np.ndarray):
                        value = value.squeeze()
                    data[field] = value
            except Exception as e2:
                print(f"Warning: Failed to load {filepath} with both h5py and scipy: {e2}")
                return None

        # Validate required features exist
        missing_features = []
        for feat in self.INPUT_FEATURES + self.OUTPUT_TRAJECTORY:
            if feat not in data:
                missing_features.append(feat)

        if missing_features:
            print(f"Warning: Missing features {missing_features} in {filepath}")

        # Handle missing quality features (may not be in old MATLAB files)
        # If missing, set to zeros or compute from existing features
        if 'internal_stress' not in data:
            data['internal_stress'] = np.zeros_like(data.get('adhesion_ratio', np.zeros(1)))
        if 'porosity' not in data:
            data['porosity'] = np.zeros_like(data.get('adhesion_ratio', np.zeros(1)))
        if 'dimensional_accuracy' not in data:
            data['dimensional_accuracy'] = np.zeros_like(data.get('adhesion_ratio', np.zeros(1)))
        if 'quality_score' not in data:
            # Compute from adhesion_ratio if available
            if 'adhesion_ratio' in data:
                data['quality_score'] = data['adhesion_ratio']  # Simple proxy
            else:
                data['quality_score'] = np.zeros(1)

        return data

    def _fit_scaler(self):
        """Fit StandardScaler on all input features"""
        # Check if we have any loaded data
        if not self.data_list:
            raise ValueError("No data loaded - all files failed to load. Please check file format and contents.")
        
        # Collect all data points
        all_features = []
        for data in self.data_list:
            if self.INPUT_FEATURES[0] in data and len(data[self.INPUT_FEATURES[0]]) > 0:
                n_samples = len(data[self.INPUT_FEATURES[0]])
                
                # Stack features, ensuring proper shape for scaler
                feature_arrays = []
                for feat in self.INPUT_FEATURES:
                    if feat in data:
                        feat_data = data[feat]
                        # Ensure the feature data is properly shaped
                        if feat_data.ndim > 1:
                            # If it's multi-dimensional, flatten or squeeze appropriately
                            feat_data = np.squeeze(feat_data)
                        feature_arrays.append(feat_data)
                    else:
                        # If feature is missing, append zeros
                        feature_arrays.append(np.zeros(n_samples))
                
                # Stack features along the last axis to form [n_samples, n_features]
                features = np.stack(feature_arrays, axis=1)
                
                # Ensure features array is 2D before appending
                if features.ndim == 2:
                    all_features.append(features)

        if not all_features:
            raise ValueError("No valid features found in loaded data - please check data format.")

        all_features = np.vstack(all_features)
        self.scaler.fit(all_features)
        print(f"Scaler fitted on {all_features.shape[0]} samples")

    def _create_sequences(self) -> List[Dict]:
        """Create input sequences and target labels"""
        sequences = []

        for data_idx, data in enumerate(self.data_list):
            # Find the minimum length across all required features to ensure consistency
            lengths = []
            for feat in self.INPUT_FEATURES + self.OUTPUT_TRAJECTORY + self.OUTPUT_QUALITY:
                if feat in data:
                    lengths.append(len(data[feat]))
            if not lengths:
                continue  # Skip if no required features are present
                
            min_length = min(lengths)
            if min_length < self.seq_len + self.pred_len:
                print(f"Warning: Data file {data_idx} has length {min_length} which is too short for required sequence length {self.seq_len + self.pred_len}, skipping...")
                continue
            
            # Create sequences
            for i in range(0, min_length - self.seq_len - self.pred_len + 1, self.stride):
                # Extract input sequence
                input_features_list = []
                for feat in self.INPUT_FEATURES:
                    feat_data = data[feat]
                    if feat_data.ndim > 1:
                        feat_data = np.squeeze(feat_data)
                    input_features_list.append(feat_data[i:i+self.seq_len])
                
                input_features = np.stack(input_features_list, axis=1)  # [seq_len, 12]

                # Extract trajectory error targets
                trajectory_targets_list = []
                for feat in self.OUTPUT_TRAJECTORY:
                    feat_data = data[feat]
                    if feat_data.ndim > 1:
                        feat_data = np.squeeze(feat_data)
                    trajectory_targets_list.append(feat_data[i+self.seq_len:i+self.seq_len+self.pred_len])
                
                trajectory_targets = np.stack(trajectory_targets_list, axis=1)  # [pred_len, 2]

                # Extract quality targets (use last timestep of prediction)
                quality_targets = []
                for feat in self.OUTPUT_QUALITY:
                    feat_data = data[feat]
                    if feat_data.ndim > 1:
                        feat_data = np.squeeze(feat_data)
                    quality_targets.append(feat_data[i+self.seq_len+self.pred_len-1])
                
                quality_targets = np.array(quality_targets)  # [5]

                sequences.append({
                    'input_features': input_features,
                    'trajectory_targets': trajectory_targets,
                    'quality_targets': quality_targets,
                    'data_idx': data_idx,
                    'start_idx': i
                })

        return sequences

    def __len__(self) -> int:
        return len(self.sequences)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """
        Get a single sample

        Returns:
            Dictionary with tensors:
            - input_features: [seq_len, 12]
            - trajectory_targets: [pred_len, 2]
            - quality_targets: [5]
        """
        seq = self.sequences[idx]

        # Normalize input features
        if self.scaler is not None:
            input_features = self.scaler.transform(seq['input_features'])
        else:
            input_features = seq['input_features']

        # Convert to tensors
        sample = {
            'input_features': torch.FloatTensor(input_features),
            'trajectory_targets': torch.FloatTensor(seq['trajectory_targets']),
            'quality_targets': torch.FloatTensor(seq['quality_targets']),
            'data_idx': seq['data_idx'],
            'start_idx': seq['start_idx']
        }

        return sample

    def get_feature_dim(self) -> int:
        """Get number of input features"""
        return len(self.INPUT_FEATURES)

    def get_trajectory_output_dim(self) -> int:
        """Get trajectory output dimension"""
        return len(self.OUTPUT_TRAJECTORY)

    def get_quality_output_dim(self) -> int:
        """Get quality output dimension"""
        return len(self.OUTPUT_QUALITY)


def create_dataloaders(train_dir: str,
                       val_dir: str,
                       test_dir: str,
                       batch_size: int = 64,
                       seq_len: int = 200,
                       pred_len: int = 50,
                       stride: int = 10,
                       num_workers: int = 4) -> Tuple[DataLoader, DataLoader, DataLoader, StandardScaler]:
    """
    Create train, validation, and test dataloaders

    Args:
        train_dir: Path to training data directory
        val_dir: Path to validation data directory
        test_dir: Path to test data directory
        batch_size: Batch size
        seq_len: Input sequence length
        pred_len: Prediction sequence length
        stride: Stride between sequences
        num_workers: Number of worker processes

    Returns:
        train_loader, val_loader, test_loader, fitted_scaler
    """
    # Create training dataset (fit scaler)
    train_dataset = PrinterSimulationDataset(
        train_dir,
        seq_len=seq_len,
        pred_len=pred_len,
        stride=stride,
        mode='train',
        scaler=None,
        fit_scaler=True
    )

    # Create validation dataset (use train scaler)
    val_dataset = PrinterSimulationDataset(
        val_dir,
        seq_len=seq_len,
        pred_len=pred_len,
        stride=stride,
        mode='val',
        scaler=train_dataset.scaler,
        fit_scaler=False
    )

    # Create test dataset (use train scaler)
    test_dataset = PrinterSimulationDataset(
        test_dir,
        seq_len=seq_len,
        pred_len=pred_len,
        stride=stride,
        mode='test',
        scaler=train_dataset.scaler,
        fit_scaler=False
    )

    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )

    print(f"\nDataset Summary:")
    print(f"  Train: {len(train_dataset)} samples, {len(train_loader)} batches")
    print(f"  Val:   {len(val_dataset)} samples, {len(val_loader)} batches")
    print(f"  Test:  {len(test_dataset)} samples, {len(test_loader)} batches")
    print(f"  Input dim:  {train_dataset.get_feature_dim()}")
    print(f"  Trajectory out: {train_dataset.get_trajectory_output_dim()}")
    print(f"  Quality out:   {train_dataset.get_quality_output_dim()}")

    return train_loader, val_loader, test_loader, train_dataset.scaler
